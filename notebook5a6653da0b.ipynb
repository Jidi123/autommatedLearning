{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install ipdb","metadata":{"execution":{"iopub.status.busy":"2022-04-20T13:54:20.876829Z","iopub.execute_input":"2022-04-20T13:54:20.877275Z","iopub.status.idle":"2022-04-20T13:56:03.175729Z","shell.execute_reply.started":"2022-04-20T13:54:20.877239Z","shell.execute_reply":"2022-04-20T13:56:03.174633Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f045cb51090>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/ipdb/\u001b[0m\n\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f045cb514d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/ipdb/\u001b[0m\n\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f045cb51890>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/ipdb/\u001b[0m\n\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f045cb51c50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/ipdb/\u001b[0m\nCollecting ipdb\n  Downloading ipdb-0.13.9.tar.gz (16 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from ipdb) (59.5.0)\nRequirement already satisfied: ipython>=7.17.0 in /opt/conda/lib/python3.7/site-packages (from ipdb) (7.30.1)\nRequirement already satisfied: toml>=0.10.2 in /opt/conda/lib/python3.7/site-packages (from ipdb) (0.10.2)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipdb) (5.1.0)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=7.17.0->ipdb) (0.7.5)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython>=7.17.0->ipdb) (0.1.3)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython>=7.17.0->ipdb) (4.8.0)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=7.17.0->ipdb) (0.2.0)\nRequirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython>=7.17.0->ipdb) (5.1.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=7.17.0->ipdb) (3.0.24)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython>=7.17.0->ipdb) (0.18.1)\nRequirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython>=7.17.0->ipdb) (2.10.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython>=7.17.0->ipdb) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\nBuilding wheels for collected packages: ipdb\n  Building wheel for ipdb (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ipdb: filename=ipdb-0.13.9-py3-none-any.whl size=11648 sha256=4e82feada23ec2f48388abdd54b5a21ba0df1756c60a61224eb9b17c74e5ff94\n  Stored in directory: /root/.cache/pip/wheels/65/cd/cc/aaf92acae337a28fdd2aa4d632196a59745c8c39f76eaeed01\nSuccessfully built ipdb\nInstalling collected packages: ipdb\nSuccessfully installed ipdb-0.13.9\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!cp -r \"/kaggle/input/dfme13/latest_experiments.txt\" /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2022-04-20T13:54:02.492992Z","iopub.execute_input":"2022-04-20T13:54:02.493260Z","iopub.status.idle":"2022-04-20T13:54:03.152958Z","shell.execute_reply.started":"2022-04-20T13:54:02.493233Z","shell.execute_reply":"2022-04-20T13:54:03.152053Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# !rm -rf \"/kaggle/output/checkpoint/\"","metadata":{"execution":{"iopub.status.busy":"2022-04-20T13:53:17.742580Z","iopub.execute_input":"2022-04-20T13:53:17.742851Z","iopub.status.idle":"2022-04-20T13:53:17.747078Z","shell.execute_reply.started":"2022-04-20T13:53:17.742815Z","shell.execute_reply":"2022-04-20T13:53:17.746342Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!cp -r \"/kaggle/input/project/datafree/dfme/checkpoint/\" /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2022-04-20T13:54:05.633317Z","iopub.execute_input":"2022-04-20T13:54:05.633930Z","iopub.status.idle":"2022-04-20T13:54:06.564716Z","shell.execute_reply.started":"2022-04-20T13:54:05.633889Z","shell.execute_reply":"2022-04-20T13:54:06.563810Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\nimport os\nimport argparse, ipdb, json\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\ncwd = os.getcwd()\nos.chdir(\"/kaggle/input/project/datafree/dfme\")\nimport network\n# os.chdir(\"/kaggle/input/dfme13/\")\nfrom dataloader import get_dataloader\nfrom approximate_gradients import *\nfrom my_utils import *\nos.chdir(cwd)\nimport os, random\nimport numpy as np\nimport torchvision\nfrom pprint import pprint\nfrom time import time\nimport torchvision.models as models\n\nprint(\"torch version\", torch.__version__)\n\ndef myprint(a):\n    \"\"\"Log the print statements\"\"\"\n    global file\n    print(a); file.write(a); file.write(\"\\n\"); file.flush()\n\n\ndef student_loss(args, s_logit, t_logit, return_t_logits=False):\n    \"\"\"Kl/ L1 Loss for student\"\"\"\n    print_logits =  False\n    if args.loss == \"l1\":\n        loss_fn = F.l1_loss\n        loss = loss_fn(s_logit, t_logit.detach())\n    elif args.loss == \"kl\":\n        loss_fn = F.kl_div\n        s_logit = F.log_softmax(s_logit, dim=1)\n        t_logit = F.softmax(t_logit, dim=1)\n        loss = loss_fn(s_logit, t_logit.detach(), reduction=\"batchmean\")\n    else:\n        raise ValueError(args.loss)\n\n    if return_t_logits:\n        return loss, t_logit.detach()\n    else:\n        return loss\n\ndef generator_loss(args, s_logit, t_logit,  z = None, z_logit = None, reduction=\"mean\"):\n    assert 0 \n    \n    loss = - F.l1_loss( s_logit, t_logit , reduction=reduction) \n    \n            \n    return loss\n\n\ndef train(args, teacher, student, generator, device, optimizer, epoch):\n    \"\"\"Main Loop for one epoch of Training Generator and Student\"\"\"\n    global file\n    teacher.eval()\n    student.train()\n    \n    optimizer_S,  optimizer_G = optimizer\n\n    gradients = []\n    \n\n    for i in range(args.epoch_itrs):\n        \"\"\"Repeat epoch_itrs times per epoch\"\"\"\n        for _ in range(args.g_iter):\n            #Sample Random Noise\n            z = torch.randn((args.batch_size, args.nz)).to(device)\n            optimizer_G.zero_grad()\n            generator.train()\n            #Get fake image from generator\n            fake = generator(z, pre_x=args.approx_grad) # pre_x returns the output of G before applying the activation\n\n\n            ## APPOX GRADIENT\n            approx_grad_wrt_x, loss_G = estimate_gradient_objective(args, teacher, student, fake, \n                                                epsilon = args.grad_epsilon, m = args.grad_m, num_classes=args.num_classes, \n                                                device=device, pre_x=True)\n\n            fake.backward(approx_grad_wrt_x)\n                \n            optimizer_G.step()\n\n            if i == 0 and args.rec_grad_norm:\n                x_true_grad = measure_true_grad_norm(args, fake)\n\n        for _ in range(args.d_iter):\n            z = torch.randn((args.batch_size, args.nz)).to(device)\n            fake = generator(z).detach()\n            optimizer_S.zero_grad()\n\n            with torch.no_grad(): \n                t_logit = teacher(fake)\n\n            # Correction for the fake logits\n            if args.loss == \"l1\" and args.no_logits:\n                t_logit = F.log_softmax(t_logit, dim=1).detach()\n                if args.logit_correction == 'min':\n                    t_logit -= t_logit.min(dim=1).values.view(-1, 1).detach()\n                elif args.logit_correction == 'mean':\n                    t_logit -= t_logit.mean(dim=1).view(-1, 1).detach()\n\n\n            s_logit = student(fake)\n\n\n            loss_S = student_loss(args, s_logit, t_logit)\n            loss_S.backward()\n            optimizer_S.step()\n\n        # Log Results\n        if i % args.log_interval == 0:\n            myprint(f'Train Epoch: {epoch} [{i}/{args.epoch_itrs} ({100*float(i)/float(args.epoch_itrs):.0f}%)]\\tG_Loss: {loss_G.item():.6f} S_loss: {loss_S.item():.6f}')\n            \n            if i == 0:\n                with open(args.log_dir + \"/loss.csv\", \"a\") as f:\n                    f.write(\"%d,%f,%f\\n\"%(epoch, loss_G, loss_S))\n\n\n            if args.rec_grad_norm and i == 0:\n\n                G_grad_norm, S_grad_norm = compute_grad_norms(generator, student)\n                if i == 0:\n                    with open(args.log_dir + \"/norm_grad.csv\", \"a\") as f:\n                        f.write(\"%d,%f,%f,%f\\n\"%(epoch, G_grad_norm, S_grad_norm, x_true_grad))\n                    \n\n        # update query budget\n        args.query_budget -= args.cost_per_iteration\n\n        if args.query_budget < args.cost_per_iteration:\n            return \n\n\ndef test(args, student = None, generator = None, device = \"cuda\", test_loader = None, epoch=0):\n    global file\n    student.eval()\n    generator.eval()\n\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for i, (data, target) in enumerate(test_loader):\n            data, target = data.to(device), target.to(device)\n            output = student(data)\n\n            test_loss += F.cross_entropy(output, target, reduction='sum').item() # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    accuracy = 100. * correct / len(test_loader.dataset)\n    myprint('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        accuracy))\n    with open(args.log_dir + \"/accuracy.csv\", \"a\") as f:\n        f.write(\"%d,%f\\n\"%(epoch, accuracy))\n    acc = correct/len(test_loader.dataset)\n    return acc\n\ndef compute_grad_norms(generator, student):\n    G_grad = []\n    for n, p in generator.named_parameters():\n        if \"weight\" in n:\n            # print('===========\\ngradient{}\\n----------\\n{}'.format(n, p.grad.norm().to(\"cpu\")))\n            G_grad.append(p.grad.norm().to(\"cpu\"))\n\n    S_grad = []\n    for n, p in student.named_parameters():\n        if \"weight\" in n:\n            # print('===========\\ngradient{}\\n----------\\n{}'.format(n, p.grad.norm().to(\"cpu\")))\n            S_grad.append(p.grad.norm().to(\"cpu\"))\n    return  np.mean(G_grad), np.mean(S_grad)\n\ndef main():\n    # Training settings\n    parser = argparse.ArgumentParser(description='DFAD CIFAR')\n    parser.add_argument('--batch_size', type=int, default=256, metavar='N',help='input batch size for training (default: 256)')\n    parser.add_argument('--query_budget', type=float, default=0.003, metavar='N', help='Query budget for the extraction attack in millions (default: 20M)')\n    parser.add_argument('--epoch_itrs', type=int, default=50)  \n    parser.add_argument('--g_iter', type=int, default=1, help = \"Number of generator iterations per epoch_iter\")\n    parser.add_argument('--d_iter', type=int, default=5, help = \"Number of discriminator iterations per epoch_iter\")\n\n    parser.add_argument('--lr_S', type=float, default=0.1, metavar='LR', help='Student learning rate (default: 0.1)')\n    parser.add_argument('--lr_G', type=float, default=1e-4, help='Generator learning rate (default: 0.1)')\n    parser.add_argument('--nz', type=int, default=256, help = \"Size of random noise input to generator\")\n\n    parser.add_argument('--log_interval', type=int, default=10, metavar='N', help='how many batches to wait before logging training status')\n    \n    parser.add_argument('--loss', type=str, default='l1', choices=['l1', 'kl'],)\n    parser.add_argument('--scheduler', type=str, default='multistep', choices=['multistep', 'cosine', \"none\"],)\n    parser.add_argument('--steps', nargs='+', default = [0.1, 0.3, 0.5], type=float, help = \"Percentage epochs at which to take next step\")\n    parser.add_argument('--scale', type=float, default=3e-1, help = \"Fractional decrease in lr\")\n\n    parser.add_argument('--dataset', type=str, default='cifar10', choices=['svhn','cifar10'], help='dataset name (default: cifar10)')\n    parser.add_argument('--data_root', type=str, default='/kaggle/working/data')\n    parser.add_argument('--model', type=str, default='resnet34_8x', choices=classifiers, help='Target model name (default: resnet34_8x)')\n    parser.add_argument('--weight_decay', type=float, default=5e-4)\n    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n                        help='SGD momentum (default: 0.9)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--seed', type=int, default=random.randint(0, 100000), metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--ckpt', type=str, default='/kaggle/working/checkpoint/teacher/cifar10-resnet34_8x.pt')\n    \n\n    parser.add_argument('--student_load_path', type=str, default=None)\n    parser.add_argument('--model_id', type=str, default=\"debug\")\n\n    parser.add_argument('--device', type=int, default=0)\n    parser.add_argument('--log_dir', type=str, default=\"/kaggle/working/save_results/cifar10\")\n\n    # Gradient approximation parameters\n    parser.add_argument('--approx_grad', type=int, default=1, help = 'Always set to 1')\n    parser.add_argument('--grad_m', type=int, default=1, help='Number of steps to approximate the gradients')\n    parser.add_argument('--grad_epsilon', type=float, default=1e-3) \n    \n\n    parser.add_argument('--forward_differences', type=int, default=1, help='Always set to 1')\n    \n\n    # Eigenvalues computation parameters\n    parser.add_argument('--no_logits', type=int, default=1)\n    parser.add_argument('--logit_correction', type=str, default='mean', choices=['none', 'mean'])\n\n    parser.add_argument('--rec_grad_norm', type=int, default=1)\n\n    parser.add_argument('--MAZE', type=int, default=0) \n\n    parser.add_argument('--store_checkpoints', type=int, default=1)\n\n    parser.add_argument('--student_model', type=str, default='resnet18_8x',\n                        help='Student model architecture (default: resnet18_8x)')\n\n\n#     args = parser.parse_args()\n    args=parser.parse_args(args=[])\n\n\n\n    args.query_budget *=  10**6\n    args.query_budget = int(args.query_budget)\n    if args.MAZE:\n\n        print(\"\\n\"*2)\n        print(\"#### /!\\ OVERWRITING ALL PARAMETERS FOR MAZE REPLCIATION ####\")\n        print(\"\\n\"*2)\n        args.scheduer = \"cosine\"\n        args.loss = \"kl\"\n        args.batch_size = 128\n        args.g_iter = 1\n        args.d_iter = 5\n        args.grad_m = 10\n        args.lr_G = 1e-4 \n        args.lr_S = 1e-1\n\n\n    if args.student_model not in classifiers:\n        if \"wrn\" not in args.student_model:\n            raise ValueError(\"Unknown model\")\n\n\n    pprint(args, width= 80)\n    print(args.log_dir)\n    os.makedirs(args.log_dir, exist_ok=True)\n\n    if args.store_checkpoints:\n        os.makedirs(args.log_dir + \"/checkpoint\", exist_ok=True)\n\n    \n    # Save JSON with parameters\n    with open(args.log_dir + \"/parameters.json\", \"w\") as f:\n        json.dump(vars(args), f)\n\n    with open(args.log_dir + \"/loss.csv\", \"w\") as f:\n        f.write(\"epoch,loss_G,loss_S\\n\")\n\n    with open(args.log_dir + \"/accuracy.csv\", \"w\") as f:\n        f.write(\"epoch,accuracy\\n\")\n\n    if args.rec_grad_norm:\n        with open(args.log_dir + \"/norm_grad.csv\", \"w\") as f:\n            f.write(\"epoch,G_grad_norm,S_grad_norm,grad_wrt_X\\n\")\n\n    with open(\"/kaggle/working/latest_experiments.txt\", \"a\") as f:\n        f.write(args.log_dir + \"\\n\")\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n\n    # Prepare the environment\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n    device = torch.device(\"cuda:%d\"%args.device if use_cuda else \"cpu\")\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    \n    # Preparing checkpoints for the best Student\n    global file\n    model_dir = f\"/kaggle/working/checkpoint/student_{args.model_id}\"; args.model_dir = model_dir\n    if(not os.path.exists(model_dir)):\n        os.makedirs(model_dir)\n    with open(f\"{model_dir}/model_info.txt\", \"w\") as f:\n        json.dump(args.__dict__, f, indent=2)  \n    file = open(f\"{args.model_dir}/logs.txt\", \"w\") \n\n    print(args)\n\n    args.device = device\n\n    # Eigen values and vectors of the covariance matrix\n    _, test_loader = get_dataloader(args)\n\n\n    args.normalization_coefs = None\n    args.G_activation = torch.tanh\n\n    num_classes = 10 if args.dataset in ['cifar10', 'svhn'] else 100\n    args.num_classes = num_classes\n\n    if args.model == 'resnet34_8x':\n        teacher = network.resnet_8x.ResNet34_8x(num_classes=num_classes)\n        if args.dataset == 'svhn':\n            print(\"Loading SVHN TEACHER\")\n            args.ckpt = 'checkpoint/teacher/svhn-resnet34_8x.pt'\n        teacher.load_state_dict( torch.load( args.ckpt, map_location=device) )\n    else:\n        teacher = get_classifier(args.model, pretrained=True, num_classes=args.num_classes)\n    \n    \n\n    teacher.eval()\n    teacher = teacher.to(device)\n    myprint(\"Teacher restored from %s\"%(args.ckpt)) \n    print(f\"\\n\\t\\tTraining with {args.model} as a Target\\n\") \n    correct = 0\n    with torch.no_grad():\n        for i, (data, target) in enumerate(test_loader):\n            data, target = data.to(device), target.to(device)\n            output = teacher(data)\n            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n    accuracy = 100. * correct / len(test_loader.dataset)\n    print('\\nTeacher - Test set: Accuracy: {}/{} ({:.4f}%)\\n'.format(correct, len(test_loader.dataset),accuracy))\n    \n    \n    \n    student = get_classifier(args.student_model, pretrained=False, num_classes=args.num_classes)\n    \n    generator = network.gan.GeneratorA(nz=args.nz, nc=3, img_size=32, activation=args.G_activation)\n\n\n    \n    student = student.to(device)\n    generator = generator.to(device)\n\n    args.generator = generator\n    args.student = student\n    args.teacher = teacher\n\n    \n    if args.student_load_path :\n        # \"checkpoint/student_no-grad/cifar10-resnet34_8x.pt\"\n        student.load_state_dict( torch.load( args.student_load_path ) )\n        myprint(\"Student initialized from %s\"%(args.student_load_path))\n        acc = test(args, student=student, generator=generator, device = device, test_loader = test_loader)\n\n    ## Compute the number of epochs with the given query budget:\n    args.cost_per_iteration = args.batch_size * (args.g_iter * (args.grad_m+1) + args.d_iter)\n\n    number_epochs = args.query_budget // (args.cost_per_iteration * args.epoch_itrs) + 1\n\n    print (f\"\\nTotal budget: {args.query_budget//1000}k\")\n    print (\"Cost per iterations: \", args.cost_per_iteration)\n    print (\"Total number of epochs: \", number_epochs)\n\n    optimizer_S = optim.SGD( student.parameters(), lr=args.lr_S, weight_decay=args.weight_decay, momentum=0.9 )\n\n    if args.MAZE:\n        optimizer_G = optim.SGD( generator.parameters(), lr=args.lr_G , weight_decay=args.weight_decay, momentum=0.9 )    \n    else:\n        optimizer_G = optim.Adam( generator.parameters(), lr=args.lr_G )\n    \n    steps = sorted([int(step * number_epochs) for step in args.steps])\n    print(\"Learning rate scheduling at steps: \", steps)\n    print()\n\n    if args.scheduler == \"multistep\":\n        scheduler_S = optim.lr_scheduler.MultiStepLR(optimizer_S, steps, args.scale)\n        scheduler_G = optim.lr_scheduler.MultiStepLR(optimizer_G, steps, args.scale)\n    elif args.scheduler == \"cosine\":\n        scheduler_S = optim.lr_scheduler.CosineAnnealingLR(optimizer_S, number_epochs)\n        scheduler_G = optim.lr_scheduler.CosineAnnealingLR(optimizer_G, number_epochs)\n\n\n    best_acc = 0\n    acc_list = []\n\n    for epoch in range(1, number_epochs + 1):\n        # Train\n        if args.scheduler != \"none\":\n            scheduler_S.step()\n            scheduler_G.step()\n        \n\n        train(args, teacher=teacher, student=student, generator=generator, device=device, optimizer=[optimizer_S, optimizer_G], epoch=epoch)\n        # Test\n        acc = test(args, student=student, generator=generator, device = device, test_loader = test_loader, epoch=epoch)\n        acc_list.append(acc)\n        if acc>best_acc:\n            best_acc = acc\n            name = 'resnet34_8x'\n            torch.save(student.state_dict(),f\"checkpoint/student_{args.model_id}/{args.dataset}-{name}.pt\")\n            torch.save(generator.state_dict(),f\"checkpoint/student_{args.model_id}/{args.dataset}-{name}-generator.pt\")\n        # vp.add_scalar('Acc', epoch, acc)\n        if args.store_checkpoints:\n            torch.save(student.state_dict(), args.log_dir + f\"/checkpoint/student.pt\")\n            torch.save(generator.state_dict(), args.log_dir + f\"/checkpoint/generator.pt\")\n    myprint(\"Best Acc=%.6f\"%best_acc)\n\n    with open(args.log_dir + \"/Max_accuracy = %f\"%best_acc, \"w\") as f:\n        f.write(\" \")\n\n     \n\n    import csv\n    os.makedirs('log', exist_ok=True)\n    with open('log/DFAD-%s.csv'%(args.dataset), 'a') as f:\n        writer = csv.writer(f)\n        writer.writerow(acc_list)\n\n\nif __name__ == '__main__':\n    main()","metadata":{"_uuid":"18b534c0-0ef1-4b1d-9f22-7b767e01c712","_cell_guid":"da78b23e-5235-488a-8425-af2f25c94025","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-20T13:56:05.892169Z","iopub.execute_input":"2022-04-20T13:56:05.892444Z","iopub.status.idle":"2022-04-20T13:56:34.357023Z","shell.execute_reply.started":"2022-04-20T13:56:05.892414Z","shell.execute_reply":"2022-04-20T13:56:34.356156Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"torch version 1.9.1\nNamespace(MAZE=0, approx_grad=1, batch_size=256, ckpt='/kaggle/working/checkpoint/teacher/cifar10-resnet34_8x.pt', d_iter=5, data_root='/kaggle/working/data', dataset='cifar10', device=0, epoch_itrs=50, forward_differences=1, g_iter=1, grad_epsilon=0.001, grad_m=1, log_dir='/kaggle/working/save_results/cifar10', log_interval=10, logit_correction='mean', loss='l1', lr_G=0.0001, lr_S=0.1, model='resnet34_8x', model_id='debug', momentum=0.9, no_cuda=False, no_logits=1, nz=256, query_budget=3000, rec_grad_norm=1, scale=0.3, scheduler='multistep', seed=21790, steps=[0.1, 0.3, 0.5], store_checkpoints=1, student_load_path=None, student_model='resnet18_8x', weight_decay=0.0005)\n/kaggle/working/save_results/cifar10\nNamespace(MAZE=0, approx_grad=1, batch_size=256, ckpt='/kaggle/working/checkpoint/teacher/cifar10-resnet34_8x.pt', d_iter=5, data_root='/kaggle/working/data', dataset='cifar10', device=0, epoch_itrs=50, forward_differences=1, g_iter=1, grad_epsilon=0.001, grad_m=1, log_dir='/kaggle/working/save_results/cifar10', log_interval=10, logit_correction='mean', loss='l1', lr_G=0.0001, lr_S=0.1, model='resnet34_8x', model_dir='/kaggle/working/checkpoint/student_debug', model_id='debug', momentum=0.9, no_cuda=False, no_logits=1, nz=256, query_budget=3000, rec_grad_norm=1, scale=0.3, scheduler='multistep', seed=21790, steps=[0.1, 0.3, 0.5], store_checkpoints=1, student_load_path=None, student_model='resnet18_8x', weight_decay=0.0005)\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /kaggle/working/data/cifar-10-python.tar.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/170498071 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"841d17d0a56641d7a0d954560d05c1d4"}},"metadata":{}},{"name":"stdout","text":"Extracting /kaggle/working/data/cifar-10-python.tar.gz to /kaggle/working/data\nFiles already downloaded and verified\nTeacher restored from /kaggle/working/checkpoint/teacher/cifar10-resnet34_8x.pt\n\n\t\tTraining with resnet34_8x as a Target\n\n\nTeacher - Test set: Accuracy: 9554/10000 (95.5400%)\n\n\nTotal budget: 3k\nCost per iterations:  1792\nTotal number of epochs:  1\nLearning rate scheduling at steps:  [0, 0, 0]\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Train Epoch: 1 [0/50 (0%)]\tG_Loss: -2.115809 S_loss: 1.570445\n\nTest set: Average loss: 2.3483, Accuracy: 1000/10000 (10.0000%)\n\nBest Acc=0.100000\n","output_type":"stream"}]}]}